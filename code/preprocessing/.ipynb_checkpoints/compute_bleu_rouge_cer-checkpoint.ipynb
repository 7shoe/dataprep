{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40fff491-a1cf-4209-afc1-1fb08aa0ab29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import ConvertParserOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3797b2c-9564-43bf-a18a-e21656823315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parser: html w/ 6 jsonl files.\n",
      "Parser: marker w/ 440 jsonl files.\n",
      "Parser: nougat w/ 388 jsonl files.\n",
      "Parser: pymupdf w/ 4 jsonl files.\n"
     ]
    }
   ],
   "source": [
    "store_path = Path('./database/decoded_text_across_parsers.csv')\n",
    "fill_na_with_empty_str:bool = False\n",
    "\n",
    "assert store_path.parent.is_dir(), \"Directory does exist\"\n",
    "\n",
    "# def creat_text_database (rows: pdf files, column parser)\n",
    "all_dict = {}\n",
    "\n",
    "# list all parsers (html = groundtruth)\n",
    "parser_name_list = ['html', 'marker', 'nougat', 'pymupdf']\n",
    "\n",
    "# loop each parser\n",
    "for parser_name in parser_name_list:\n",
    "\n",
    "    # grab HTML\n",
    "    p_html = p_jsonl_root / f'joint_to_{parser_name}/parsed_pdfs'\n",
    "    jsonl_files = [p_html / f for f in os.listdir(p_html)]\n",
    "    \n",
    "    # each parser\n",
    "    pdf_path_list = []\n",
    "    pdf_text_list = []\n",
    "\n",
    "    # status\n",
    "    print(f'Parser: {parser_name} w/ {len(jsonl_files)} jsonl files.')\n",
    "\n",
    "    # jsonl\n",
    "    for jsonl_file in jsonl_files:\n",
    "        # open\n",
    "        with open(jsonl_file, 'r') as f:\n",
    "            for line in f:\n",
    "                data = json.loads(line)\n",
    "                # extract path / text\n",
    "                pdf_path = data['path']\n",
    "                decoded_text = data['text']\n",
    "                # append \n",
    "                pdf_path_list.append(pdf_path)\n",
    "                pdf_text_list.append(decoded_text)\n",
    "\n",
    "    # append to to dict\n",
    "    all_dict[parser_name] = {'path' : pdf_path_list, 'text' : pdf_text_list}\n",
    "\n",
    "# index : available PDFs (sorted by parent, filename)\n",
    "all_pdf_paths = [item for sublist in [all_dict[k]['path'] for k in all_dict.keys()] for item in sublist]\n",
    "index_set  = {pdf for pdf in all_pdf_paths}\n",
    "index_list = [Path(idx) for idx in index_set if '.ipynb_checkpoints' not in idx]\n",
    "sorted_index_list = sorted(index_list, key=lambda p: (Path(p).parent, Path(p).name))\n",
    "\n",
    "\n",
    "# setup DataFrame\n",
    "df = pd.DataFrame(index=sorted_index_list, columns=parser_name_list)\n",
    "\n",
    "# Iterate over parser_name_list\n",
    "for parser_name in parser_name_list:\n",
    "    # Extract the corresponding dictionary from all_dict\n",
    "    paths = all_dict[parser_name]['path']\n",
    "    texts = all_dict[parser_name]['text']\n",
    "    \n",
    "    # Create a temporary DataFrame with paths as the index and texts as the data\n",
    "    temp_df = pd.DataFrame(texts, index=paths, columns=[parser_name])\n",
    "    \n",
    "    # Update the main DataFrame df with the data from temp_df\n",
    "    df.update(temp_df)\n",
    "\n",
    "# Optionally, fill any remaining NaN values with a default value (e.g., empty string)\n",
    "if fill_na_with_empty_str:\n",
    "    df.fillna('', inplace=True)\n",
    "\n",
    "# store\n",
    "df.to_csv(store_path, sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809d1670-275d-4a66-be70-4a77759533e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./database/decoded_text_across_parsers.csv', sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e33f51-7164-4c36-ad45-52ab65e0d8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLEU score\n",
    "bleu_score = sentence_bleu([reference], candidate)\n",
    "print(f'BLEU Score: {bleu_score:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b454d47-6e69-49ed-815d-96e4ca09e532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROUGE score\n",
    "\n",
    "reference = \"This is a test.\"\n",
    "candidate = \"This is test.\"\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "scores = scorer.score(reference, candidate)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1f583c-7a06-4fe0-8df1-f3aee984d407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CER\n",
    "\n",
    "reference = \"This unique is a test\"\n",
    "candidate = \"This is test\"\n",
    "\n",
    "cer = jiwer.cer(reference, candidate)\n",
    "print(f'CER: {cer:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b557353e-4529-40e7-b65b-08ff8d82b615",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bo",
   "language": "python",
   "name": "bo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
