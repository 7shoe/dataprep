{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1bcc4be-873e-457b-b25b-dc9b8dc5ddce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nature_urls import nature_urls\n",
    "from pathlib import Path\n",
    "import time\n",
    "import random\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4d90a06-c73d-4af5-916e-180096749301",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Nature_Spyder:\n",
    "    def __init__(self, url_list:list[str|Path]):\n",
    "        # Initialize the Chrome driver\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument('--headless')\n",
    "        self.driver = webdriver.Chrome(options=options)\n",
    "        self.urls = url_list\n",
    "        self.articles_urls = []\n",
    "        \n",
    "        self.title_list = []\n",
    "        self.article_url_list = []\n",
    "        self.description_list = []\n",
    "        self.authors_list = []\n",
    "        self.publication_date_list = []\n",
    "        self.access_type_list = []\n",
    "\n",
    "        #random.shuffle(self.urls)\n",
    "\n",
    "    def scroll_down(self, pixels):\n",
    "        ActionChains(self.driver).scroll_by_amount(0, pixels).perform()\n",
    "    \n",
    "    def scrape_articles_info(self, url):\n",
    "        # Open the target URL\n",
    "        self.driver.get(url)\n",
    "\n",
    "        # Find all article elements on the page\n",
    "        article_elements = self.driver.find_elements(By.CSS_SELECTOR, \"article.u-full-height\")\n",
    "\n",
    "        title_list = []\n",
    "        article_url_list = []\n",
    "        description_list = []\n",
    "        authors_list = []\n",
    "        publication_date_list = []\n",
    "        access_type_list = []\n",
    "        \n",
    "        # Iterate over each article element and extract information\n",
    "        for article in article_elements:\n",
    "            try:\n",
    "                # Extract the title\n",
    "                title_element = article.find_element(By.CSS_SELECTOR, \"h3.c-card__title a\")\n",
    "                title = title_element.text\n",
    "                title_list.append(title)\n",
    "                \n",
    "                # Extract the article URL\n",
    "                article_url = title_element.get_attribute('href')\n",
    "                article_url_list.append(article_url)\n",
    "                \n",
    "                # Extract the description/summary\n",
    "                description_element = article.find_element(By.CSS_SELECTOR, \"div.c-card__summary p\")\n",
    "                description = description_element.text\n",
    "                description_list.append(description)\n",
    "\n",
    "                # Extract the authors\n",
    "                author_elements = article.find_elements(By.CSS_SELECTOR, \"ul.c-author-list li span[itemprop='name']\")\n",
    "                authors = [author.text for author in author_elements]\n",
    "                authors_list.append(authors)\n",
    "\n",
    "                # Extract the publication date\n",
    "                date_element = article.find_element(By.CSS_SELECTOR, \"time.c-meta__item\")\n",
    "                publication_date = date_element.get_attribute('datetime')\n",
    "                publication_date_list.append(publication_date)\n",
    "\n",
    "                # Extract the access type\n",
    "                access_element = article.find_element(By.CSS_SELECTOR, \"span.u-color-open-access\")\n",
    "                access_type = access_element.text\n",
    "                access_type_list.append(access_type)\n",
    "\n",
    "                # Print the scraped information\n",
    "                print(f\"Title: {title}\")\n",
    "                #print(f\"URL: {article_url}\")\n",
    "                #print(f\"Description: {description}\")\n",
    "                #print(f\"Authors: {', '.join(authors)}\")\n",
    "                #print(f\"Publication Date: {publication_date}\")\n",
    "                #print(f\"Access Type: {access_type}\")\n",
    "                #print(\"=\" * 40)\n",
    "            except Exception as e:\n",
    "                print(f\"Error scraping article: {e}\")\n",
    "\n",
    "        # Close the driver\n",
    "        self.driver.quit()\n",
    "\n",
    "        return title_list, article_url_list, description_list, authors_list, publication_date_list, access_type_list\n",
    "\n",
    "    def scrape_all(self,) -> None:\n",
    "        for url_loc in self.urls:\n",
    "            time.sleep(12.)\n",
    "            try:\n",
    "                title_list, article_url_list, description_list, authors_list, publication_date_list, access_type_list = self.scrape_articles_info(url=url_loc)\n",
    "                self.title_list += title_list\n",
    "                self.article_url_list += article_url_list\n",
    "                self.description_list += description_list\n",
    "                self.authors_list += authors_list\n",
    "                self.publication_date_list += publication_date_list\n",
    "                self.access_type_list += access_type_list\n",
    "\n",
    "            except Exception as e:\n",
    "                print('Skip')\n",
    "                print(f'url: {url_loc}, SKIPPED due to error {e}')\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54388b6e-8c1c-4669-ba24-5bf50a83c06e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random.seed(35)\n",
    "for i in range(500):\n",
    "    # wait\n",
    "    random.shuffle(nature_urls)\n",
    "    nature_subset_urls = nature_urls[:2]\n",
    "    time.sleep(15)\n",
    "    \n",
    "    # scrape\n",
    "    spyder = Nature_Spyder(nature_subset_urls)\n",
    "    spyder.scrape_all()\n",
    "    \n",
    "    # store\n",
    "    df_nature = pd.DataFrame({'html_url' : spyder.article_url_list})\n",
    "    df_nature.to_csv('./registry/nature_database.csv', sep='|', mode='a', header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b95c657-11d5-4192-8b47-b05cfa03f6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09329ace-921f-47c3-80e2-30b2ccd5dca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./registry/nature_database.csv', sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4e88513-9eea-411e-99d6-cbbfdb061637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "420"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.drop_duplicates(subset=['html_url']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7e83114-3f0f-4c43-93c3-c9feff40d706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "860"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2bee42-4629-4603-9cb1-dab170314923",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bo",
   "language": "python",
   "name": "bo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
