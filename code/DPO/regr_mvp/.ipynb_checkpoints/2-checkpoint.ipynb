{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e9597bd-771f-4335-9b57-32a2dc82517b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c24d010-c0da-4ffb-9e3f-2c7b5bbe1ef0",
   "metadata": {},
   "source": [
    "## Setup Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba75670c-7f84-431d-b77b-82457064e5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "output_dim = 11\n",
    "\n",
    "# Load your model (initially for classification)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=output_dim)\n",
    "\n",
    "# Define LoRA configuration for multivariate regression\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS, # sequenceClass as helper\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    ")\n",
    "\n",
    "# Apply the LoRA PEFT configuration to your model\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "# Modify output dimension for regression\n",
    "model.config.num_labels = output_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc52e37-692b-4d50-9fe0-f7eb92029708",
   "metadata": {},
   "source": [
    "## Dataset & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d2c34c6-c92f-4fdd-bcb1-dbb9eb11bae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example dataset class\n",
    "class RandomRegressionDataset(Dataset):\n",
    "    def __init__(self, size=1000, seq_length=128, output_dim=11):\n",
    "        self.size = size\n",
    "        self.seq_length = seq_length\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = torch.randint(0, 30522, (self.seq_length,))  # Random token ids\n",
    "        attention_mask = torch.ones(self.seq_length)  # Dummy attention mask\n",
    "        labels = torch.randn(self.output_dim)  # Random regression targets\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
    "\n",
    "# create random dataset for training and evaluation\n",
    "train_dataset = RandomRegressionDataset()\n",
    "eval_dataset = RandomRegressionDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7cd997a-0189-4a20-a87d-7a85bd6fa4d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m7shoe\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/siebenschuh/Projects/dataprep/code/DPO/regr_mvp/wandb/run-20240914_015928-hgewazjj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/7shoe/huggingface/runs/hgewazjj' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/7shoe/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/7shoe/huggingface' target=\"_blank\">https://wandb.ai/7shoe/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/7shoe/huggingface/runs/hgewazjj' target=\"_blank\">https://wandb.ai/7shoe/huggingface/runs/hgewazjj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='315' max='315' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [315/315 00:27, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.016000</td>\n",
       "      <td>1.011762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.014000</td>\n",
       "      <td>1.002350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.961900</td>\n",
       "      <td>0.984955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.994200</td>\n",
       "      <td>1.002096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.002900</td>\n",
       "      <td>1.005605</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=315, training_loss=1.0160121826898485, metrics={'train_runtime': 32.4899, 'train_samples_per_second': 153.894, 'train_steps_per_second': 9.695, 'total_flos': 330080340480000.0, 'train_loss': 1.0160121826898485, 'epoch': 5.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# custom Trainer to handle the regression loss (MSE)\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        predictions = outputs.logits\n",
    "        loss_fn = torch.nn.MSELoss()  # Mean Squared Error Loss\n",
    "        loss = loss_fn(predictions, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",  # Use eval_strategy instead of deprecated evaluation_strategy\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,  # Shortened for testing purposes\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Trainer setup using CustomTrainer\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d7701e-48f9-46a3-858f-c566c0634030",
   "metadata": {},
   "source": [
    "## Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c542ad89-1a79-436e-bd91-a20c70eb7644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a subset of the evaluation dataset for inference\n",
    "subset_size = 23  # Use a small subset for testing\n",
    "eval_subset = [eval_dataset[i] for i in range(subset_size)]\n",
    "\n",
    "# Convert the subset to a batch for inference\n",
    "input_ids = torch.stack([item['input_ids'] for item in eval_subset])\n",
    "attention_mask = torch.stack([item['attention_mask'] for item in eval_subset])\n",
    "labels = torch.stack([item['labels'] for item in eval_subset])\n",
    "\n",
    "inputs = {\n",
    "    'input_ids': input_ids,\n",
    "    'attention_mask': attention_mask\n",
    "}\n",
    "\n",
    "# Move inputs to the appropriate device (GPU/CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "labels = labels.to(device)\n",
    "\n",
    "# Run inference\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    predictions = outputs.logits  # These are the predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd1f7d0c-b02f-4e60-9d68-5ffe180b6151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on subset: 1.0689997673034668\n"
     ]
    }
   ],
   "source": [
    "# mean Squared Error computation\n",
    "mse_loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "# calculate MSE between predictions and true labels\n",
    "mse = mse_loss_fn(predictions, labels)\n",
    "\n",
    "print(f\"Mean Squared Error on subset: {mse.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0d816b9-8b80-49e0-ac3a-0f6d323500a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 8.3236e-02, -7.4599e-02, -5.5312e-02, -6.8720e-05,  3.2924e-02,\n",
       "          1.1646e-01,  7.6541e-02,  3.1326e-02, -7.0111e-03,  1.4391e-02,\n",
       "          6.4919e-02],\n",
       "        [ 5.5729e-02, -9.6576e-02, -7.5652e-02,  2.7464e-02,  1.7071e-02,\n",
       "          1.5047e-01,  7.6762e-02,  4.8520e-02, -1.8715e-02,  2.4906e-02,\n",
       "          6.8290e-02],\n",
       "        [ 2.2446e-02,  1.8741e-02, -4.6552e-02, -7.7787e-04,  1.5758e-02,\n",
       "          2.9205e-02, -2.7818e-02,  3.7431e-02,  1.2304e-02,  2.8333e-02,\n",
       "         -3.7628e-03],\n",
       "        [ 8.7305e-04,  2.7116e-02, -5.6817e-02, -3.2062e-02,  1.9789e-02,\n",
       "         -8.7455e-03, -2.3889e-02, -1.6129e-02,  3.3801e-02,  1.3483e-02,\n",
       "         -1.4406e-02],\n",
       "        [ 5.9830e-03,  2.5881e-02, -3.5188e-02,  7.4200e-03,  4.3201e-04,\n",
       "          2.2960e-02, -3.3125e-02,  3.2310e-02, -1.2279e-02, -5.6384e-04,\n",
       "         -8.7629e-03],\n",
       "        [ 6.3883e-02, -6.7322e-02, -4.3836e-02,  1.4952e-02, -1.9108e-03,\n",
       "          1.1069e-01,  5.2055e-02,  5.7284e-02, -2.0236e-02, -1.2038e-02,\n",
       "          4.2569e-02],\n",
       "        [ 2.2942e-02, -2.0897e-02, -3.1968e-02,  8.8983e-03, -5.4392e-03,\n",
       "          7.2479e-02,  3.8502e-03,  5.5141e-02, -5.0900e-03,  1.6507e-03,\n",
       "          1.6206e-02],\n",
       "        [ 1.3658e-03,  2.8361e-02, -8.1826e-03,  9.6057e-03, -1.1007e-02,\n",
       "          5.3039e-02, -1.7391e-02,  4.9215e-02, -8.9127e-03, -2.1025e-03,\n",
       "         -4.1502e-04],\n",
       "        [-3.4163e-03,  1.9194e-03, -3.0732e-02,  9.0155e-04,  2.8690e-02,\n",
       "          2.4243e-02, -1.8506e-02,  3.1580e-02,  2.2211e-02,  4.7387e-03,\n",
       "          1.1131e-02],\n",
       "        [ 5.5733e-02, -7.7259e-02, -5.0312e-02, -9.1123e-04,  1.7688e-02,\n",
       "          1.1652e-01,  6.7263e-02,  3.3018e-02,  1.5782e-02,  1.6236e-02,\n",
       "          4.4378e-02],\n",
       "        [ 2.9093e-02,  6.5133e-03, -4.4444e-02,  5.9703e-03,  3.2565e-02,\n",
       "          5.9266e-02, -4.0928e-04,  4.5401e-02,  2.2745e-02,  3.9371e-02,\n",
       "          2.7089e-02],\n",
       "        [ 3.0740e-02, -1.7108e-03, -3.0762e-02, -2.4999e-02,  4.8368e-02,\n",
       "          2.9360e-02,  1.4301e-02,  7.4323e-03,  1.2468e-02,  4.0960e-02,\n",
       "          1.4905e-02],\n",
       "        [ 1.6533e-03,  1.6740e-03, -1.9078e-02,  5.2425e-03, -1.9609e-02,\n",
       "          3.4383e-02, -9.4760e-03,  4.9864e-02, -1.6074e-02, -1.4734e-02,\n",
       "          1.4031e-02],\n",
       "        [ 1.9856e-02, -3.0862e-02, -2.9241e-02, -1.4257e-02,  4.7976e-02,\n",
       "          5.5176e-02,  1.0181e-02,  3.8149e-02,  3.3012e-02,  3.7119e-02,\n",
       "          4.1771e-02],\n",
       "        [ 7.3390e-03,  1.0924e-02, -3.7772e-02, -1.4267e-02,  1.7047e-02,\n",
       "          2.1136e-02, -1.1066e-02,  3.2948e-02,  1.0098e-02, -7.1265e-04,\n",
       "          1.0920e-02],\n",
       "        [ 5.9071e-02, -3.6779e-02, -5.8631e-02,  1.9585e-02,  3.2004e-02,\n",
       "          1.0128e-01,  3.9628e-02,  2.9828e-02,  3.9149e-02,  2.2767e-02,\n",
       "          6.3916e-02],\n",
       "        [ 6.5379e-02, -3.8338e-02, -7.6342e-02,  1.6398e-02,  3.0909e-02,\n",
       "          9.8739e-02,  3.2059e-02,  2.7662e-02,  2.1333e-02,  4.9770e-02,\n",
       "          4.3551e-02],\n",
       "        [-4.6168e-03, -1.3588e-02, -3.5640e-02,  5.5148e-03,  2.1009e-02,\n",
       "          2.8684e-02, -9.5803e-03,  5.1019e-02,  1.3744e-02,  4.9943e-02,\n",
       "          3.3108e-03],\n",
       "        [-4.1628e-03,  5.7205e-02, -5.5379e-02,  1.6572e-02,  5.5175e-02,\n",
       "         -2.7345e-02, -3.2130e-02,  1.6696e-03,  2.9920e-02,  5.0278e-02,\n",
       "         -1.4219e-02],\n",
       "        [ 4.0634e-02, -3.0292e-02, -4.8579e-02,  1.6531e-02,  3.4878e-02,\n",
       "          5.5672e-02,  1.6261e-02,  3.2975e-02,  7.2895e-03,  3.7437e-02,\n",
       "          2.4446e-02],\n",
       "        [ 9.8952e-03, -2.7099e-02, -3.1635e-02,  1.1220e-02,  6.9849e-03,\n",
       "          2.1885e-02, -4.9683e-03,  5.1015e-02, -2.2078e-03, -4.5811e-03,\n",
       "          9.3123e-03],\n",
       "        [ 3.2694e-03,  5.1276e-02, -5.7507e-02,  2.6636e-02,  3.7522e-02,\n",
       "         -4.5026e-02, -2.6164e-02,  1.7742e-02, -7.4294e-03,  3.0755e-03,\n",
       "         -2.5736e-02],\n",
       "        [-3.3146e-02,  2.6957e-02, -2.7991e-02,  2.0920e-02, -3.0424e-02,\n",
       "          1.1069e-02, -5.9881e-02,  6.3939e-02, -1.8489e-02, -2.4689e-02,\n",
       "         -1.7493e-02]], device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d9b051-851b-43e4-baab-df45b360c271",
   "metadata": {},
   "source": [
    "## Decision Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d2809f-d186-4489-9684-e0de43c7555c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_function(predictions:torch.Tensor):\n",
    "    '''\n",
    "    Make decision\n",
    "    '''\n",
    "    return predictions.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2ab1e5-950a-41c0-8a05-d5755d1cdde3",
   "metadata": {},
   "source": [
    "## Store model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7cb7f4-51a8-4ca3-b173-4a00c7a6d829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the path where you want to save the model\n",
    "model_save_path = \"./stored_regression\"\n",
    "\n",
    "# save the fine-tuned model along with LoRA adapters\n",
    "model.save_pretrained(model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c1d636-dfad-4faa-b54e-6105de9565cd",
   "metadata": {},
   "source": [
    "# LOADING SCRIPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "38a9012c-8f9a-42ea-9c37-e44cfc95ee9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "# load the base model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=11)\n",
    "\n",
    "# load the fine-tuned model with LoRA adapters\n",
    "model_save_path = '/home/siebenschuh/Projects/dataprep/code/DPO/regr_mvp/stored_regression'\n",
    "model = PeftModel.from_pretrained(model, model_save_path)\n",
    "\n",
    "# move the model to the appropriate device if necessary\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def decision_function(predictions:torch.Tensor):\n",
    "    '''\n",
    "    Make decision\n",
    "    '''\n",
    "    return predictions.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabc12ef-9b91-44b5-9185-60391b102c74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dpo",
   "language": "python",
   "name": "dpo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
