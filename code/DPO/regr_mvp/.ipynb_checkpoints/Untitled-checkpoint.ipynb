{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "417fd93a-0389-468b-bbef-5865da0a787b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2108e468-21da-4556-997a-2ee7fdb383d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.27754348516464233\n"
     ]
    }
   ],
   "source": [
    "# Load a pretrained model and tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "pretrained_model = AutoModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Configure LoRA using LoraConfig\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # Rank of the LoRA update matrices\n",
    "    lora_alpha=32,  # Alpha scaling factor\n",
    "    target_modules=[\"query\", \"value\"],  # Apply LoRA to the attention layers (query, value)\n",
    "    lora_dropout=0.1,  # Dropout for LoRA layers\n",
    "    bias=\"none\"  # Do not modify bias terms\n",
    ")\n",
    "\n",
    "# Apply the LoRA configuration to the pretrained model\n",
    "peft_model = get_peft_model(pretrained_model, lora_config)\n",
    "\n",
    "# Define a simple regression head for multivariate regression\n",
    "class RegressionHead(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(RegressionHead, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)  # output_dim is the size of the numeric vector\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Add the regression head to the LoRA-adapted model\n",
    "class TextToVectorModel(nn.Module):\n",
    "    def __init__(self, peft_model, output_dim):\n",
    "        super(TextToVectorModel, self).__init__()\n",
    "        self.peft_model = peft_model\n",
    "        self.regression_head = RegressionHead(self.peft_model.config.hidden_size, output_dim)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Get the hidden states from the PEFT model\n",
    "        outputs = self.peft_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Apply the regression head to the [CLS] token's hidden state\n",
    "        hidden_state = outputs.last_hidden_state[:, 0, :]\n",
    "        return self.regression_head(hidden_state)\n",
    "\n",
    "# Instantiate the model for a specific output vector size (e.g., 10-dimensional vector)\n",
    "output_dim = 10\n",
    "model_with_head = TextToVectorModel(peft_model, output_dim)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error Loss (sum of squares)\n",
    "optimizer = optim.AdamW(model_with_head.parameters(), lr=1e-5)\n",
    "\n",
    "# Example input\n",
    "text = \"This is a sample input text.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "# Forward pass\n",
    "outputs = model_with_head(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n",
    "\n",
    "# Target numeric vector (random for example purposes)\n",
    "target_vector = torch.rand(1, output_dim)\n",
    "\n",
    "# Compute loss\n",
    "loss = criterion(outputs, target_vector)\n",
    "\n",
    "# Backpropagation\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print(f\"Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81cea1d-0393-46cf-8a3b-1bf06f0c5e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_function(outputs:torch.Tensor):\n",
    "    '''\n",
    "    Decision function\n",
    "    '''\n",
    "    y = torch.max(outputs) > 0.5\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f305eaf5-1a38-4389-a991-2a8f79b10542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: 'Hello world, this is me'\n",
      "Predicted Vector: [ 0.11075529  0.43679807 -0.2734996  -0.2975242   0.2994047   0.10984623\n",
      "  0.2022312   0.0246725   0.0659114  -0.06034581]\n",
      "\n",
      "Text: 'Oh no I didn't know this was possible!'\n",
      "Predicted Vector: [ 0.08057702  0.59461147 -0.2719262  -0.18875416  0.2057085   0.41809207\n",
      "  0.18287022  0.10906797  0.4716116  -0.08316108]\n",
      "\n",
      "Text: 'Last input here we go'\n",
      "Predicted Vector: [ 0.14999892  0.62819505 -0.05891944  0.07449528  0.33450708  0.31278065\n",
      "  0.48138773 -0.1258212   0.26548895 -0.25189307]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List of texts for inference\n",
    "texts = [\"Hello world, this is me\", \n",
    "         \"Oh no I didn't know this was possible!\", \n",
    "         \"Last input here we go\"]\n",
    "\n",
    "# Function to perform inference on a list of texts\n",
    "def infer_text_to_vector(model, tokenizer, texts, output_dim):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():  # No need to compute gradients during inference\n",
    "        for text in texts:\n",
    "            # Tokenize the input text\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "            \n",
    "            # Perform a forward pass to get the predicted vector\n",
    "            outputs = model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n",
    "            \n",
    "            # Convert the outputs to a NumPy array and store the prediction\n",
    "            predictions.append(outputs.squeeze().cpu().numpy())\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Call the inference function\n",
    "predicted_vectors = infer_text_to_vector(model_with_head, tokenizer, texts, output_dim)\n",
    "\n",
    "# Print the resulting vectors for each input text\n",
    "for i, vector in enumerate(predicted_vectors):\n",
    "    print(f\"Text: '{texts[i]}'\\nPredicted Vector: {vector}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e6f55c0-1703-4ace-8fd3-31f0e1ac83db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d80beef-5ddf-401f-bb5e-4f43ee9b3daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved to ./stored_regression\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the directory where the model will be stored\n",
    "save_directory = \"./stored_regression\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(save_directory):\n",
    "    os.makedirs(save_directory)\n",
    "\n",
    "# Save the PEFT model and tokenizer\n",
    "model_with_head.peft_model.save_pretrained(save_directory)\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "\n",
    "print(f\"Model and tokenizer saved to {save_directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "712add71-f1fe-48da-931f-926e2face9d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer successfully loaded!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "# Load the tokenizer\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
    "\n",
    "# Load the model\n",
    "loaded_peft_model = AutoModel.from_pretrained(save_directory)\n",
    "peft_model = PeftModel.from_pretrained(loaded_peft_model, save_directory)\n",
    "\n",
    "# Re-create the full model with the regression head\n",
    "loaded_model_with_head = TextToVectorModel(peft_model, output_dim)\n",
    "\n",
    "print(\"Model and tokenizer successfully loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e662d869-5bdc-401e-98ee-0a73a504a951",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dpo",
   "language": "python",
   "name": "dpo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
