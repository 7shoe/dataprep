{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0790845a-c80d-453b-9832-e0ca9e40a16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "from datasets import load_from_disk, concatenate_datasets, Dataset\n",
    "from data_utils import compile_DatasetFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a616759-d058-45a3-8404-e93d42f69fbf",
   "metadata": {},
   "source": [
    "## Preliminary Analysis:\n",
    "- $n \\approx 15,000$ (slightly low)\n",
    "- $N_{str} \\leq 1,600$ (low-ish maximum number of characters considered)\n",
    "- embeddings (if used) from `gist` (moderately sized model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebd2c38-4938-41d8-bb0d-e8af7192f4e4",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ae15c0-cd33-48ac-bc9c-ead64e4320a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load frames\n",
    "df_train, df_test, df_val = compile_DatasetFrames()\n",
    "\n",
    "# convert to datasets\n",
    "dset_train = Dataset.from_pandas(df_train)\n",
    "#dset_test = Dataset.from_pandas(df_test) # ignore test for a bit\n",
    "dset_val = Dataset.from_pandas(df_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410b9df5-2e20-4ebf-aa6d-7dc4ccee485e",
   "metadata": {},
   "source": [
    "### Model: `peft`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fb6a78-993c-43dd-bf36-9a55aa34c9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "peft_config = LoraConfig(task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1)\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"bigscience/mt0-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a0495d-c85e-46b8-930f-eaea03955379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aaeff82-57f1-4fa5-8521-f9de59f469d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Preprocess the dataset\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True)\n",
    "\n",
    "# Step 2: Tokenize the dataset\n",
    "dset_train_encoded = dset_train.map(preprocess_function, batched=True)\n",
    "dset_val_encoded = dset_val.map(preprocess_function, batched=True)\n",
    "\n",
    "# Remove unnecessary columns\n",
    "dset_train_encoded = dset_train_encoded.remove_columns(['text', 'embeddings', 'path', 'abstract', 'firstpage', '__index_level_0__'])\n",
    "dset_val_encoded = dset_val_encoded.remove_columns(['text', 'embeddings', 'path', 'abstract', 'firstpage', '__index_level_0__'])\n",
    "\n",
    "# Set the label column\n",
    "dset_train_encoded = dset_train_encoded.rename_column('journal_cls', 'labels')\n",
    "dset_val_encoded = dset_val_encoded.rename_column('journal_cls', 'labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac0800a-36b6-40ef-80e7-961aef9b144f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set format for PyTorch\n",
    "dset_train_encoded.set_format(\"torch\")\n",
    "dset_val_encoded.set_format(\"torch\")\n",
    "\n",
    "# Step 3: Load the model\n",
    "num_labels = len(set(dset_train['journal_cls']))  # Number of unique classes\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "\n",
    "# Step 4: Set up LoRA config\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,  # Sequence classification task\n",
    "    inference_mode=False,\n",
    "    r=8,  # The rank of the low-rank matrices\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1\n",
    ")\n",
    "\n",
    "# Step 5: Apply LoRA to the base model using PEFT\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "# Data collator (for dynamic padding)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Step 6: Define metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    return {\"accuracy\": acc}\n",
    "\n",
    "# Step 7: Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# Step 8: Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dset_train_encoded,\n",
    "    eval_dataset=dset_val_encoded,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Step 9: Train the model with LoRA applied\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322fb2e4-068e-4cfd-a82c-4298256408cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dpo",
   "language": "python",
   "name": "dpo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
