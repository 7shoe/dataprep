{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cf0ae915-6a57-4628-be1d-d18f08000bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('disk I/O error')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "import socket\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6882d1a8-90e9-4800-9344-c89388bc4ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BioRXiV_Meta_Creator:\n",
    "    def __init__(self, \n",
    "                 biorxiv_src_html_path:Path = Path('/eagle/projects/argonne_tpc/siebenschuh/aurora_gpt/merged_raw_data/bioarxiv/html'),\n",
    "                 meta_dst_csv_dir:Path = Path('/eagle/projects/argonne_tpc/siebenschuh/aurora_gpt/meta_tables/biorxiv_meta.csv')):\n",
    "\n",
    "        # check paths\n",
    "        meta_dst_csv_dir = Path(meta_dst_csv_dir)\n",
    "        assert meta_dst_csv_dir.parent.is_dir(), f\"`meta_dst_csv_dir` parent directory does not exist: Invalid {meta_dir.parent}\"\n",
    "        self.meta_dst_csv_dir = meta_dst_csv_dir\n",
    "\n",
    "        biorxiv_src_html_path = Path(biorxiv_src_html_path)\n",
    "        assert biorxiv_src_html_path.is_dir(), f\"`biorxiv_src_html_path` parent directory does not exist: Invalid {meta_dst_csv_dir}\"\n",
    "        self.biorxiv_src_html_path = biorxiv_src_html_path\n",
    "        \n",
    "        \n",
    "        pass\n",
    "\n",
    "    def get_abstract(self, driver):\n",
    "        \"\"\"\n",
    "        Extracts abstract from BioRXiv HTML website\n",
    "        \"\"\"\n",
    "        # Extract the abstract\n",
    "        try:\n",
    "            abstract_element = driver.find_element(By.CSS_SELECTOR, \"div.section.abstract\")\n",
    "            abstract_text = abstract_element.text.replace('|', '')\n",
    "            return abstract_text\n",
    "        except Exception as e:\n",
    "            return \"\"\n",
    "    \n",
    "    def get_title(self, driver):\n",
    "        \"\"\"\n",
    "        Extracts title from BioRXiv HTML website\n",
    "        \"\"\"\n",
    "        # Extract the title\n",
    "        try:\n",
    "            title_element = driver.find_element(By.CSS_SELECTOR, \"h1.highwire-cite-title\")\n",
    "            title_text = title_element.text.replace('|', '')\n",
    "            return title_text\n",
    "        except Exception as e:\n",
    "            return \"\"\n",
    "    \n",
    "    def get_published_date(self, driver):\n",
    "        \"\"\"\n",
    "        Extracts the published date from a BioRXiv HTML website\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Find the element that contains the published date\n",
    "            date_element = driver.find_element(By.CSS_SELECTOR, \"div.panel-pane.pane-custom.pane-1 .pane-content\")\n",
    "            \n",
    "            # Get the text from the element\n",
    "            date_text = date_element.text\n",
    "            \n",
    "            # Extract the date by removing \"Posted\" and any surrounding whitespace\n",
    "            date_text = date_text.replace('Posted', '').strip().split('.')[0]\n",
    "    \n",
    "            # process \n",
    "            date_obj = datetime.strptime(date_text.strip(), \"%B %d, %Y\")\n",
    "        \n",
    "            # Convert the datetime object to the desired format\n",
    "            formatted_date = date_obj.strftime(\"%d-%m-%Y\")\n",
    "            \n",
    "            # Return the cleaned-up date text\n",
    "            return formatted_date\n",
    "        except Exception as e:\n",
    "            return \"\"\n",
    "    \n",
    "    def get_doi(self, driver):\n",
    "        \"\"\"\n",
    "        Extracts the DOI from a BioRXiv HTML website\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Find the element that contains the DOI\n",
    "            doi_element = driver.find_element(By.CSS_SELECTOR, \"span.highwire-cite-metadata-doi.highwire-cite-metadata\")\n",
    "            \n",
    "            # Get the text from the element\n",
    "            doi_text = doi_element.text.strip()\n",
    "    \n",
    "            # format\n",
    "            doi_text = doi_text.split('doi: ')[1]\n",
    "            \n",
    "            # Return the DOI text\n",
    "            return doi_text\n",
    "        except Exception as e:\n",
    "            return \"\"\n",
    "\n",
    "    def process_file(self, html_path):\n",
    "        # driver options \n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument('--headless')\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        \n",
    "        # Load the HTML file\n",
    "        driver.get(f\"file://{html_path}\")\n",
    "        html_content = driver.page_source\n",
    "        doc_len = len(html_content)\n",
    "    \n",
    "        # extract\n",
    "        title_scraped = get_title(driver)\n",
    "        abstract_scraped = get_abstract(driver)\n",
    "        date_scraped = get_published_date(driver)\n",
    "        doi_scraped = get_doi(driver)\n",
    "        \n",
    "        # paths\n",
    "        p_html = html_path\n",
    "        p_pdf = str(html_path).replace('/html/', '/pdf/').replace('.html', '.pdf')\n",
    "        p_csv = str(html_path).replace('/html/', '/csv/').replace('.html', '.csv')\n",
    "        \n",
    "        # Assuming `df` is a predefined DataFrame with metadata\n",
    "        tmp_dict = {k:v for k,v in zip(df.iloc[:,0], df.iloc[:,1])}\n",
    "        tmp_dict['p_html'] = p_html\n",
    "        tmp_dict['p_pdf'] = p_pdf\n",
    "        tmp_dict['p_csv'] = p_csv\n",
    "        tmp_dict['title_scraped'] = title_scraped\n",
    "        tmp_dict['abstract_scraped'] = abstract_scraped\n",
    "        tmp_dict['date_scraped'] = date_scraped\n",
    "        tmp_dict['doi_scraped'] = doi_scraped\n",
    "    \n",
    "        driver.quit()\n",
    "    \n",
    "        # Only append if both PDF and CSV files exist\n",
    "        if Path(p_pdf).is_file() and Path(p_csv).is_file():\n",
    "            return tmp_dict\n",
    "        return None\n",
    "    \n",
    "    def store_biorxiv_meta(self, ):\n",
    "        '''Compile meta data in a parellized fashion\n",
    "        '''\n",
    "        # Path to BioRXiv metadata\n",
    "        html_file_paths = [self.biorxiv_src_html_path / f for f in os.listdir(self.biorxiv_src_html_path) if f.endswith('.html')]\n",
    "    \n",
    "        # debug\n",
    "        html_file_paths = html_file_paths[:100]\n",
    "        \n",
    "        meta_list = []\n",
    "    \n",
    "        # Use ProcessPoolExecutor to run tasks in parallel\n",
    "        with ProcessPoolExecutor() as executor:\n",
    "            results = executor.map(process_file, html_file_paths)\n",
    "            \n",
    "            # Filter out None results and append valid ones\n",
    "            for result in results:\n",
    "                if result:\n",
    "                    meta_list.append(result)\n",
    "    \n",
    "        # Convert list to DataFrame\n",
    "        df_meta = pd.DataFrame(meta_list)\n",
    "\n",
    "        # store\n",
    "        df_meta.to_csv(self.meta_dst_csv_dir, sep='|')\n",
    "        \n",
    "        return df_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ad214e29-a2ea-4029-97a4-2050e8e6b289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"div.panel-pane.pane-custom.pane-1 .pane-content\"}\n",
      "  (Session info: chrome-headless-shell=127.0.6533.99); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "#0 0x56027a4be6aa <unknown>\n",
      "#1 0x56027a18f600 <unknown>\n",
      "#2 0x56027a1debcb <unknown>\n",
      "#3 0x56027a1deeb1 <unknown>\n",
      "#4 0x56027a222b24 <unknown>\n",
      "#5 0x56027a2018cd <unknown>\n",
      "#6 0x56027a22004a <unknown>\n",
      "#7 0x56027a201643 <unknown>\n",
      "#8 0x56027a1d1d31 <unknown>\n",
      "#9 0x56027a1d279e <unknown>\n",
      "#10 0x56027a48623b <unknown>\n",
      "#11 0x56027a48a1d2 <unknown>\n",
      "#12 0x56027a4735f5 <unknown>\n",
      "#13 0x56027a48ad62 <unknown>\n",
      "#14 0x56027a45823f <unknown>\n",
      "#15 0x56027a4ade48 <unknown>\n",
      "#16 0x56027a4ae020 <unknown>\n",
      "#17 0x56027a4bd47c <unknown>\n",
      "#18 0x14cf21bba6ea start_thread\n",
      "\n",
      "Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"div.panel-pane.pane-custom.pane-1 .pane-content\"}\n",
      "  (Session info: chrome-headless-shell=127.0.6533.99); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "#0 0x555709c526aa <unknown>\n",
      "#1 0x555709923600 <unknown>\n",
      "#2 0x555709972bcb <unknown>\n",
      "#3 0x555709972eb1 <unknown>\n",
      "#4 0x5557099b6b24 <unknown>\n",
      "#5 0x5557099958cd <unknown>\n",
      "#6 0x5557099b404a <unknown>\n",
      "#7 0x555709995643 <unknown>\n",
      "#8 0x555709965d31 <unknown>\n",
      "#9 0x55570996679e <unknown>\n",
      "#10 0x555709c1a23b <unknown>\n",
      "#11 0x555709c1e1d2 <unknown>\n",
      "#12 0x555709c075f5 <unknown>\n",
      "#13 0x555709c1ed62 <unknown>\n",
      "#14 0x555709bec23f <unknown>\n",
      "#15 0x555709c41e48 <unknown>\n",
      "#16 0x555709c42020 <unknown>\n",
      "#17 0x555709c5147c <unknown>\n",
      "#18 0x14f73c8406ea start_thread\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resource temporarily unavailable (src/thread.cpp:241)\n",
      "Resource temporarily unavailable (src/thread.cpp:241)\n",
      "Resource temporarily unavailable (src/thread.cpp:241)\n",
      "Resource temporarily unavailable (src/thread.cpp:241)\n"
     ]
    },
    {
     "ename": "BrokenProcessPool",
     "evalue": "A process in the process pool was terminated abruptly while the future was running or pending.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBrokenProcessPool\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:2\u001b[0m\n",
      "Cell \u001b[0;32mIn[65], line 142\u001b[0m, in \u001b[0;36mBioRXiV_Meta_Creator.store_biorxiv_meta\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    139\u001b[0m results \u001b[38;5;241m=\u001b[39m executor\u001b[38;5;241m.\u001b[39mmap(process_file, html_file_paths)\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m# Filter out None results and append valid ones\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresults\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmeta_list\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/concurrent/futures/process.py:620\u001b[0m, in \u001b[0;36m_chain_from_iterable_of_lists\u001b[0;34m(iterable)\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_chain_from_iterable_of_lists\u001b[39m(iterable):\n\u001b[1;32m    615\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;124;03m    Specialized implementation of itertools.chain.from_iterable.\u001b[39;00m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;124;03m    Each item in *iterable* should be a list.  This function is\u001b[39;00m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;124;03m    careful not to keep references to yielded objects.\u001b[39;00m\n\u001b[1;32m    619\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43melement\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m        \u001b[49m\u001b[43melement\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreverse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mwhile\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43melement\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/concurrent/futures/_base.py:619\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[1;32m    618\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 619\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    620\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    621\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs\u001b[38;5;241m.\u001b[39mpop(), end_time \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic())\n",
      "File \u001b[0;32m/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/concurrent/futures/_base.py:317\u001b[0m, in \u001b[0;36m_result_or_cancel\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 317\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    319\u001b[0m         fut\u001b[38;5;241m.\u001b[39mcancel()\n",
      "File \u001b[0;32m/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/concurrent/futures/_base.py:456\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    458\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mBrokenProcessPool\u001b[0m: A process in the process pool was terminated abruptly while the future was running or pending."
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "m = BioRXiV_Meta_Creator()\n",
    "m.store_biorxiv_meta()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac5be0b-03dc-4a1f-a281-c696c3658862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check output\n",
    "df_read = pd.read_csv('/eagle/projects/argonne_tpc/siebenschuh/aurora_gpt/meta_tables/', sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a9ba876c-d63b-40e4-8ae5-a425dc54e458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dir `meta`\n",
    "# write meta data into a single CSV\n",
    "# columns: `title`, `authors`, `journal`, `doi` (unique format?), `date_published`, `abstract`\n",
    "# get metadata from Biorxiv\n",
    "\n",
    "def get_biorxiv_meta():\n",
    "    # Path to BioRXiv metadata\n",
    "    biorxiv_path    = Path('/eagle/projects/argonne_tpc/siebenschuh/aurora_gpt/merged_raw_data/bioarxiv/html')\n",
    "    html_file_paths = [biorxiv_path / f for f in os.listdir(biorxiv_path) if f.endswith('.html')]\n",
    "    \n",
    "    # driver options \n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless')\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    \n",
    "    meta_list = []\n",
    "\n",
    "    # debug\n",
    "    html_file_paths = html_file_paths[:100]\n",
    "    \n",
    "    # fill-up\n",
    "    for _,html_path in enumerate(html_file_paths):\n",
    "        # load\n",
    "        driver.get(f\"file://{html_path}\")\n",
    "        html_content = driver.page_source\n",
    "        doc_len = len(html_content)\n",
    "        \n",
    "        # extract\n",
    "        title_scraped = get_title(driver)\n",
    "        abstract_scraped = get_abstract(driver)\n",
    "        date_scraped = get_published_date(driver)\n",
    "        doi_scraped = get_doi(driver)\n",
    "    \n",
    "        # paths\n",
    "        p_html = html_path\n",
    "        p_pdf = str(html_path).replace('/html/', '/pdf/').replace('.html', '.pdf')\n",
    "        p_csv = str(html_path).replace('/html/', '/csv/').replace('.html', '.csv')\n",
    "    \n",
    "        # extract meta data\n",
    "        tmp_dict = {k:v for k,v in zip(df.iloc[:,0], df.iloc[:,1])}\n",
    "        tmp_dict['p_html'] = p_html\n",
    "        tmp_dict['p_pdf'] = p_pdf\n",
    "        tmp_dict['p_csv'] = p_csv\n",
    "        tmp_dict['len'] = doc_len\n",
    "        tmp_dict['title_scraped'] = title_scraped\n",
    "        tmp_dict['abstract_scraped'] = abstract_scraped\n",
    "        tmp_dict['date_scraped'] = date_scraped\n",
    "        tmp_dict['doi_scraped'] = doi_scraped\n",
    "    \n",
    "        # append\n",
    "        if Path(p_pdf).is_file() and Path(p_csv).is_file():\n",
    "            meta_list.append(tmp_dict)\n",
    "    \n",
    "    driver.quit()\n",
    "\n",
    "    # \n",
    "    #df_meta = pd.DataFrame(meta_list)\n",
    "    return df_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5dc50fb1-008f-4668-88e0-e0df0131f7f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "178d4517-8103-429d-8427-b994b89b08a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 54.6 ms, sys: 273 ms, total: 328 ms\n",
      "Wall time: 24.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_meta = get_biorxiv_meta_parallel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7b31c409-7fb3-4eef-ba76-04fe1beb7100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 529 ms, sys: 50.4 ms, total: 579 ms\n",
      "Wall time: 1min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_meta = get_biorxiv_meta()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e4f31f-dcfe-4626-bbe7-04abcd7a3837",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bo",
   "language": "python",
   "name": "bo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
