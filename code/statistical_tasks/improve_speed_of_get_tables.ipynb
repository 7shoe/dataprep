{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d073de8-fdd0-4739-9307-e55dd967131a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /eagle/projects/tpc/siebenschuh/envs_/bo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "import evaluate\n",
    "\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# DEBUG\n",
    "from nltk import translate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a29421-db39-4f6c-8e75-2f9f495932d4",
   "metadata": {},
   "source": [
    "# Script that explored alternative ways to accelerate computation of BLEU/ROUGE/CAR etc. that tends to be slow\n",
    "\n",
    "- lesson learned: naÃ¯ve `pandas` implementation (parallelize over rows) was not worse that elaborate schemes in `dask`\n",
    "\n",
    "## Speed Assessment\n",
    "`get_tables.py` has been a real bottleneck.\n",
    "We need 25k rows with 6 parsers (Nougat, PyMuPDF, Marker, Oreo, pypdf, Grobid). \n",
    "\n",
    "Compute as many metrics as pssobiel (BLEU, ROUGE, EDIT). \n",
    "\n",
    "25k Rows for 3 * 6 metrics to parse in less than 1 hour $\\rightarrow$ 5s per 100 rows per metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28a39d25-7afb-4cd5-b506-847c49c88bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastResponseTable:\n",
    "    def __init__(self,\n",
    "                 db_src_filename:str,\n",
    "                 db_dst_filename:str,\n",
    "                 chunk_index:int=-1,\n",
    "                 chunk_size:int=-1,\n",
    "                 num_cores:int=100,\n",
    "                 overwrite_flag:bool = False,\n",
    "                 parser_columns:list[str]=['pymupdf', 'nougat', 'grobid'],\n",
    "                 root_dir:Path=Path('/lus/eagle/projects/argonne_tpc/siebenschuh/aurora_gpt/database'),\n",
    "                 n:int=-1) -> None:\n",
    "        \"\"\"\n",
    "        Assume `html` is groudntruth text\n",
    "        \"\"\"\n",
    "\n",
    "        # validate\n",
    "        root_dir = Path(root_dir)\n",
    "        assert root_dir.is_dir(), f\"Path`root_dir` does not exist or is not directory. Invalid directory: {root_dir}\"\n",
    "        \n",
    "        # paths\n",
    "        self.db_src_path = Path(root_dir) / db_src_filename\n",
    "        self.db_dst_path = Path(root_dir) / db_dst_filename\n",
    "        self.overwrite_flag = overwrite_flag\n",
    "        self.n = n\n",
    "        self.parser_columns = parser_columns\n",
    "        self.chunk_index = chunk_index\n",
    "        self.chunk_size = chunk_size\n",
    "        self.num_cores = num_cores\n",
    "        \n",
    "        # raw data\n",
    "        assert self.db_src_path.is_file(), f\"Source CSV path invalid. No such path: {self.db_src_path}\"\n",
    "        if not(self.overwrite_flag):\n",
    "            assert not(self.db_dst_path.is_file()), f\"Destination Path invalid. File already exists at path: {self.db_dst_path}\"\n",
    "        \n",
    "        # load df\n",
    "        df_raw = pd.read_csv(self.db_src_path, sep='|')\n",
    "        \n",
    "        # drop NA rows\n",
    "        df_proc = df_raw.dropna()\n",
    "\n",
    "        # subset DataFrame\n",
    "        if self.chunk_index!=-1 and self.chunk_size!=-1:\n",
    "            # check validity \n",
    "            assert self.chunk_index >= 0, f\"`chunk_index` should be non-negative (or -1) but is {chunk_index}\"\n",
    "            assert self.chunk_size > 0, f\"`chunk_size` should be positive (or -1) but is {chunk_size}\"\n",
    "            \n",
    "            # overwrite self.db_dst_path\n",
    "            db_dst_filename = Path(db_dst_filename).stem + f'_{self.chunk_index}-{len(df_proc) // self.chunk_size}' + Path(db_dst_filename).suffix\n",
    "            self.db_dst_path = Path(root_dir) / db_dst_filename\n",
    "            \n",
    "            # subset dataframe\n",
    "            i_start, i_end = chunk_index * chunk_size, min((chunk_index+1) * chunk_size, len(df_proc))\n",
    "            if i_start>= len(df_proc):\n",
    "                raise ValueError(f'i_start index exceeds length of Dataframe: i_start={i_start} for len(df_proc)={len(df_proc)}')\n",
    "            df_proc = df_proc.iloc[i_start:i_end]\n",
    "\n",
    "            #  DEBUG\n",
    "            print(f'len(df_proc): {len(df_proc)}, i_start/i_end: ', i_start, i_end)\n",
    "\n",
    "        # status\n",
    "        print(f'DF loaded...\\n{len(df_raw)} rows\\n... {len(df_proc)} after removing NANs & subsetting')\n",
    "        \n",
    "        # subset\n",
    "        if n > 0:\n",
    "            df_proc = df_proc.iloc[:round(n)]\n",
    "            print(f'n={n} ... Only use first n rows.')\n",
    "        \n",
    "        # normalized text columns\n",
    "        df_proc['html_norm'] = df_proc.apply(lambda row: self.normalize(row['html']), axis=1)\n",
    "        for parser_col in self.parser_columns:\n",
    "            df_proc[f'{parser_col}_norm'] = df_proc.apply(lambda row: self.normalize(row[parser_col]), axis=1)\n",
    "\n",
    "        # latex text columns\n",
    "        df_proc['html_latex'] = df_proc.apply(lambda row: self.extract_latex(row['html']), axis=1)\n",
    "        for parser_col in self.parser_columns:\n",
    "            df_proc[f'{parser_col}_latex'] = df_proc.apply(lambda row: self.extract_latex(row[parser_col]), axis=1)\n",
    "            \n",
    "        # assign\n",
    "        self.df = df_proc\n",
    "\n",
    "        pass\n",
    "\n",
    "    def extract_latex(self, text):\n",
    "        \"\"\"\n",
    "        Extracts LaTeX expressions from the input text and returns both the LaTeX expressions and the text with LaTeX stripped.\n",
    "    \n",
    "        Parameters:\n",
    "            text (str): The input string containing LaTeX expressions.\n",
    "    \n",
    "        Returns:\n",
    "            tuple: A tuple containing two elements:\n",
    "                - A list of extracted LaTeX expressions.\n",
    "                - A string with the LaTeX expressions removed.\n",
    "        \"\"\"\n",
    "        # Regular expression to match LaTeX expressions\n",
    "        latex_pattern = re.compile(r'(\\$.*?\\$|\\\\\\[.*?\\\\\\]|\\\\\\(.*?\\\\\\)|\\\\begin\\{.*?\\}.*?\\\\end\\{.*?\\})', re.DOTALL)\n",
    "    \n",
    "        # Extract all LaTeX expressions\n",
    "        latex_expressions = latex_pattern.findall(text)\n",
    "    \n",
    "        # Strip the LaTeX expressions from the text\n",
    "        stripped_text = latex_pattern.sub('', text).strip()\n",
    "    \n",
    "        return latex_expressions\n",
    "        \n",
    "    def remove_latex(self,\n",
    "                     text:str) -> str:\n",
    "        \"\"\"\n",
    "        Remove LaTeX formatting from a string\n",
    "        \"\"\"\n",
    "        # Remove LaTeX commands (e.g., \\textbf{...}, \\emph{...}, etc.)\n",
    "        text = re.sub(r'\\\\[a-zA-Z]+\\{(.*?)\\}', r'\\1', text)\n",
    "        \n",
    "        # Remove inline math (e.g., $...$)\n",
    "        text = re.sub(r'\\$(.*?)\\$', r'\\1', text)\n",
    "        \n",
    "        # Remove display math (e.g., \\[...\\] or $$...$$)\n",
    "        text = re.sub(r'\\$\\$(.*?)\\$\\$', r'\\1', text)\n",
    "        text = re.sub(r'\\\\\\[(.*?)\\\\\\]', r'\\1', text)\n",
    "        \n",
    "        # Remove other LaTeX-specific characters (e.g., \\, \\%, etc.)\n",
    "        text = re.sub(r'\\\\[a-zA-Z]+', '', text)\n",
    "        \n",
    "        # Remove braces and any content between them\n",
    "        text = re.sub(r'\\{|\\}', '', text)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def normalize(self,\n",
    "                  x:str, \n",
    "                  remove_latex_flag:bool=True) -> str:\n",
    "        \"\"\"\n",
    "        Normalize the text\n",
    "        \"\"\"\n",
    "    \n",
    "        # const\n",
    "        REMOVE_PUNCT = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    \n",
    "        # remove latex\n",
    "        if remove_latex_flag:\n",
    "            x = self.remove_latex(x)\n",
    "    \n",
    "        # remove escape characters\n",
    "        x = x.translate(REMOVE_PUNCT)\n",
    "        x = re.sub(r\"\\s+\", \" \", x)\n",
    "        x = x.lower()\n",
    "        x = x.strip()\n",
    "        \n",
    "        return x\n",
    "\n",
    "    # - NLTK tokenization\n",
    "    def safe_word_tokenize(self, text):\n",
    "        if pd.isna(text):\n",
    "            return []\n",
    "        \n",
    "        return word_tokenize(text)\n",
    "    \n",
    "    def parallel_tokenize(self, series):\n",
    "        \"\"\"\n",
    "        (Safe) tokenization of entire column of a df\n",
    "        \"\"\"\n",
    "        \n",
    "        with Pool(processes=self.num_cores) as pool:  # Explicitly set the number of processes\n",
    "            return pool.map(self.safe_word_tokenize, series)\n",
    "\n",
    "    def extract_parts(self, token_list):\n",
    "        \"\"\"\n",
    "        Extract sublist (of tokens) post tokenization of the text column\n",
    "        \"\"\"\n",
    "        \n",
    "        length = len(token_list)\n",
    "\n",
    "        # extract first, middle, and last 10%\n",
    "        first_10 = token_list[:max(1, length // 10)]\n",
    "        mid_start = length // 2 - max(1, length // 20)\n",
    "        mid_end = length // 2 + max(1, length // 20)\n",
    "        middle_10 = token_list[mid_start:mid_end]\n",
    "        last_10 = token_list[-max(1, length // 10):]\n",
    "        \n",
    "        return first_10, middle_10, last_10\n",
    "        \n",
    "    def tokenize_columns(self,):\n",
    "        \"\"\"\n",
    "        Tokenize html/parser text columns in dataframe\n",
    "        \"\"\"\n",
    "\n",
    "        # get current df\n",
    "        df_proc = pd.DataFrame(self.df)\n",
    "\n",
    "        # parser outputs\n",
    "        for parser_col in ['html'] + self.parser_columns:\n",
    "            print(f'Tokenizing {parser_col} ... ')\n",
    "            df_proc[f'{parser_col}_token'] = self.parallel_tokenize(df_proc[parser_col])\n",
    "            df_proc[f'{parser_col}_norm_token'] = self.parallel_tokenize(df_proc[f'{parser_col}_norm'])\n",
    "            print('... completed!\\n')\n",
    "\n",
    "        # append beginning/middle/end part\n",
    "        for parser_col in ['html'] + self.parser_columns:\n",
    "            df_proc[f'{parser_col}Beg_token'], df_proc[f'{parser_col}Mid_token'], df_proc[f'{parser_col}End_token'] = zip(*df_proc[f'{parser_col}_token'].apply(self.extract_parts))\n",
    "            df_proc[f'{parser_col}Beg_norm_token'], df_proc[f'{parser_col}Mid_norm_token'], df_proc[f'{parser_col}End_norm_token'] = zip(*df_proc[f'{parser_col}_norm_token'].apply(self.extract_parts))\n",
    "        \n",
    "        # re-assign\n",
    "        self.df = df_proc\n",
    "\n",
    "        pass\n",
    "    \n",
    "    # - BLEU\n",
    "    def compute_bleu(self, row, reference_col, candidate_col):\n",
    "        return translate.bleu_score.sentence_bleu(\n",
    "            [row[reference_col]], \n",
    "            row[candidate_col], \n",
    "            smoothing_function=translate.bleu_score.SmoothingFunction().method1\n",
    "        )\n",
    "    \n",
    "    # Function to apply compute_bleu in parallel\n",
    "    def parallel_bleu(self, df, reference_col, candidate_col):\n",
    "        with Pool(processes=self.num_cores) as pool:\n",
    "            results = pool.starmap(self.compute_bleu, [(row, reference_col, candidate_col) for _, row in df.iterrows()])\n",
    "        return results\n",
    "    \n",
    "    # - METEOR SLOW\n",
    "    def compute_meteor(self, row, reference_col, candidate_col):\n",
    "        return translate.meteor_score.meteor_score(\n",
    "            [row[reference_col]], \n",
    "            row[candidate_col]\n",
    "        )\n",
    "    \n",
    "    # Function to apply compute_meteor in parallel\n",
    "    def parallel_meteor(self, df, reference_col, candidate_col):\n",
    "        with Pool(processes=self.num_cores) as pool:\n",
    "            results = pool.starmap(self.compute_meteor, [(row, reference_col, candidate_col) for _, row in df.iterrows()])\n",
    "        return results\n",
    "\n",
    "    # Function to compute the approximate CER (1 minus CER)\n",
    "    def compute_approx_car(self, row, reference_col, candidate_col):\n",
    "        '''\n",
    "        Complement of character error rate (CER); hence: character accuracy rate (CAR)\n",
    "        '''\n",
    "        similarity = fuzz.ratio(row[reference_col], row[candidate_col])\n",
    "        return similarity / 100.0\n",
    "    \n",
    "    # Function to apply compute_approx_cer in parallel\n",
    "    def parallel_car(self, df, reference_col, candidate_col):\n",
    "        '''\n",
    "        Complement of character error rate (CER); hence: character accuracy rate\n",
    "        '''\n",
    "        with Pool(processes = self.num_cores) as pool:\n",
    "            results = pool.starmap(self.compute_approx_car, [(row, reference_col, candidate_col) for _, row in df.iterrows()])\n",
    "        return results\n",
    "\n",
    "    # ROUGE scores (rouge1, rouge2, rougeL) for a single row\n",
    "    def compute_rouge(self, row, reference_col, candidate_col, scorer):\n",
    "        scores = scorer.score(row[reference_col], row[candidate_col])\n",
    "        return [scores['rouge1'].fmeasure, scores['rouge2'].fmeasure, scores['rougeL'].fmeasure]\n",
    "    \n",
    "    # Function to apply compute_rouge in parallel\n",
    "    def parallel_rouge(self, df, reference_col, candidate_col, scorer):\n",
    "        with Pool(processes=self.num_cores) as pool:\n",
    "            results = pool.starmap(self.compute_rouge, [(row, reference_col, candidate_col, scorer) for _, row in df.iterrows()])\n",
    "        return results\n",
    "        \n",
    "\n",
    "    def compute_metrics(self, normalized: bool = True) -> None:\n",
    "        \"\"\"\n",
    "        Processes the table in parallel\n",
    "        \"\"\"\n",
    "        \n",
    "        # Copy frame of processed columns\n",
    "        df_proc = pd.DataFrame(self.df)\n",
    "        \n",
    "        # Initialize a dictionary to store new columns\n",
    "        new_columns = {}\n",
    "    \n",
    "        # BLEU\n",
    "        print('BLEU ...')\n",
    "        for parser_col in self.parser_columns:\n",
    "            print(f'   {parser_col}')\n",
    "            for part in ['', 'Beg', 'Mid', 'End']:\n",
    "                new_columns[f'{parser_col}{part}_bleu'] = self.parallel_bleu(df_proc, f'html{part}_token', f'{parser_col}_token')\n",
    "                new_columns[f'{parser_col}{part}_bleu_norm'] = self.parallel_bleu(df_proc, f'html{part}_norm_token', f'{parser_col}_norm_token')\n",
    "        print('...done')\n",
    "    \n",
    "        # ROUGE\n",
    "        print('ROUGE ...')\n",
    "        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "        for parser_col in self.parser_columns:\n",
    "            print(f'   {parser_col}')\n",
    "            for part in ['']:\n",
    "                # raw text\n",
    "                rouge_scores = self.parallel_rouge(df_proc, f'html{part}', f'{parser_col}{part}', scorer)\n",
    "                # - assign\n",
    "                new_columns[f'{parser_col}{part}_rouge1'] = [score[0] for score in rouge_scores]\n",
    "                new_columns[f'{parser_col}{part}_rouge2'] = [score[1] for score in rouge_scores]\n",
    "                new_columns[f'{parser_col}{part}_rougeL'] = [score[2] for score in rouge_scores]\n",
    "        \n",
    "                # normalized text\n",
    "                rouge_scores_norm = self.parallel_rouge(df_proc, f'html{part}_norm', f'{parser_col}{part}_norm', scorer)\n",
    "                # - assign\n",
    "                new_columns[f'{parser_col}{part}_rouge1_norm'] = [score_norm[0] for score_norm in rouge_scores_norm]\n",
    "                new_columns[f'{parser_col}{part}_rouge2_norm'] = [score_norm[1] for score_norm in rouge_scores_norm]\n",
    "                new_columns[f'{parser_col}{part}_rougeL_norm'] = [score_norm[2] for score_norm in rouge_scores_norm]\n",
    "        print('...done')\n",
    "    \n",
    "        # CAR (Character Accuracy Rate)\n",
    "        print('CAR ...')\n",
    "        for parser_col in self.parser_columns:\n",
    "            print(f'   {parser_col}')\n",
    "            for part in ['', 'Beg', 'Mid', 'End']:\n",
    "                new_columns[f'{parser_col}{part}_car'] = self.parallel_car(df_proc, f'html{part}_token', f'{parser_col}{part}_token')\n",
    "                new_columns[f'{parser_col}{part}_car_norm'] = self.parallel_car(df_proc, f'html{part}_norm_token', f'{parser_col}{part}_norm_token')\n",
    "        print('...done')\n",
    "    \n",
    "        # Convert the new columns dictionary to a DataFrame\n",
    "        new_columns_df = pd.DataFrame(new_columns)\n",
    "    \n",
    "        # Concatenate the new columns to df_proc\n",
    "        df_proc = pd.concat([df_proc, new_columns_df], axis=1)\n",
    "    \n",
    "        # Assign the final DataFrame to self.df_metrics\n",
    "        self.df_metrics = df_proc\n",
    "\n",
    "        pass\n",
    "\n",
    "    def save_table(self,) -> None:\n",
    "        \"\"\"Store processed table\n",
    "        \"\"\"\n",
    "        \n",
    "        # store table\n",
    "        self.df_score.to_csv(self.db_dst_path, sep='|')\n",
    "\n",
    "        pass \n",
    "\n",
    "    def load_table(self,) -> None:\n",
    "        \"\"\"Store processed table\n",
    "        \"\"\"\n",
    "        assert self.db_dst_path.is_file(), \"\"\n",
    "        # store table\n",
    "        self.df_score = pd.read_csv(self.db_dst_path, sep='|')\n",
    "\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28324255-e20e-4175-a7e2-4172dbad3047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(df_proc): 100, i_start/i_end:  0 100\n",
      "DF loaded...\n",
      "500 rows\n",
      "... 100 after removing NANs & subsetting\n"
     ]
    }
   ],
   "source": [
    "t = FastResponseTable(root_dir='./tmp',\n",
    "                      chunk_index=0,\n",
    "                      chunk_size=100,\n",
    "                      num_cores=8,\n",
    "                      db_src_filename='df_500.csv', \n",
    "                      db_dst_filename='out_df_500.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e64b1c8-f3cb-4751-a63d-c6d5a3f80251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing html ... \n",
      "... completed!\n",
      "\n",
      "Tokenizing pymupdf ... \n",
      "... completed!\n",
      "\n",
      "Tokenizing nougat ... \n",
      "... completed!\n",
      "\n",
      "Tokenizing grobid ... \n",
      "... completed!\n",
      "\n",
      "CPU times: user 3.74 s, sys: 10.9 s, total: 14.6 s\n",
      "Wall time: 31.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "t.tokenize_columns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff2169d-d89e-45db-80dc-b4ecc524b395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU ...\n",
      "   pymupdf\n",
      "   nougat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-273:\n",
      "Process ForkPoolWorker-278:\n",
      "Process ForkPoolWorker-279:\n",
      "Process ForkPoolWorker-275:\n",
      "Process ForkPoolWorker-276:\n",
      "Process ForkPoolWorker-277:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/multiprocessing/queues.py\", line 365, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "  File \"/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/multiprocessing/connection.py\", line 430, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "          ^^^^^^^^^^^^^\n",
      "  File \"/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/multiprocessing/connection.py\", line 395, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "Process ForkPoolWorker-280:\n",
      "Process ForkPoolWorker-274:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "                    ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/multiprocessing/pool.py\", line 50, in starmapstar\n",
      "    def starmapstar(args):\n",
      "    \n",
      "  File \"/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "t.compute_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f98e31-75a8-4862-a47b-8d193a1e5ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca974776-2988-43ad-a5c2-a2fed1b40300",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bo",
   "language": "python",
   "name": "bo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
